{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import dirname, abspath\n",
    "from shutil import copy2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import ImageEnhance\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from data import dataset\n",
    "\n",
    "from preprocessing.constants import TRAINING_PATH, TESTING_PATH\n",
    "from data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI16\n"
     ]
    }
   ],
   "source": [
    "print(\"HI16\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset contains 8478 images\n",
      "SIZE =  8478\n",
      "Training Dataset contains 8255 images\n",
      "SIZE =  8255\n"
     ]
    }
   ],
   "source": [
    "training_dataset = Dataset(TRAINING_PATH, name=\"Training\", height=400, width=400, batch_size=30)\n",
    "testing_dataset = Dataset(TESTING_PATH, name=\"Training\", height=400, width=400, batch_size=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7f8130c22f40>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnidentifiedImageError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-1453221def1f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtraining_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_labels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Developer/BaldiLab/Visual-Acuity/data/dataset.py\u001B[0m in \u001B[0;36mprocess_labels\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mfile\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".png\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m                     \u001B[0mabs_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m                     \u001B[0mfile_title\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtitle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimg_to_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mload_img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mabs_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001B[0m in \u001B[0;36mload_img\u001B[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001B[0m\n\u001B[1;32m    298\u001B[0m       \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0minterpolation\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msupported\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    299\u001B[0m   \"\"\"\n\u001B[0;32m--> 300\u001B[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001B[0m\u001B[1;32m    301\u001B[0m                         target_size=target_size, interpolation=interpolation)\n\u001B[1;32m    302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001B[0m in \u001B[0;36mload_img\u001B[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001B[0m\n\u001B[1;32m    112\u001B[0m                           'The use of `load_img` requires PIL.')\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpil_image\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBytesIO\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcolor_mode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'grayscale'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m             \u001B[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   2956\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mmessage\u001B[0m \u001B[0;32min\u001B[0m \u001B[0maccept_warnings\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2957\u001B[0m         \u001B[0mwarnings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2958\u001B[0;31m     raise UnidentifiedImageError(\n\u001B[0m\u001B[1;32m   2959\u001B[0m         \u001B[0;34m\"cannot identify image file %r\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mfilename\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mfp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2960\u001B[0m     )\n",
      "\u001B[0;31mUnidentifiedImageError\u001B[0m: cannot identify image file <_io.BytesIO object at 0x7f8130c22f40>"
     ]
    }
   ],
   "source": [
    "training_dataset.process_labels()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"~~~~~~~~~~~~\")\n",
    "print(\"   Train    \")\n",
    "print(\"~~~~~~~~~~~~\")\n",
    "print(\"y_train_optotype: \", training_dataset.optotype)\n",
    "print(\"y_train_angle: \", training_dataset.angle)\n",
    "\n",
    "print(\"~~~~~~~~~~~~\")\n",
    "print(\"    Test    \")\n",
    "print(\"~~~~~~~~~~~~\")\n",
    "print(\"y_test_optotype: \", testing_dataset.optotype)\n",
    "print(\"y_test_angle: \", np.unique(training_dataset.angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def print_random_test_values(ax):\n",
    "    index = np.random.randint(0, x_test.shape[0])\n",
    "    print(\"~~~~~~~~~~~~\")\n",
    "    print(\"Test Values \")\n",
    "    print(\"~~~~~~~~~~~~\")\n",
    "\n",
    "    print(\"Index = {x}\".format(x=index))\n",
    "\n",
    "    print(\"Optotype = {x}\".format(x=y_test_optotype[index]))\n",
    "    print(\"Angle = {x}\".format(x=y_test_angle[index]))\n",
    "    print(x_test[index])\n",
    "\n",
    "    ax.set_title(\"Test- {x}\".format(x=y_test_optotype[index]))\n",
    "    ax.imshow(x_test[index])\n",
    "\n",
    "def print_random_training_values(ax):\n",
    "    index = np.random.randint(0, x_train.shape[0])\n",
    "    print(\"~~~~~~~~~~~~\")\n",
    "    print(\"Train Values\")\n",
    "    print(\"~~~~~~~~~~~~\")\n",
    "\n",
    "    print(\"Index = {x}\".format(x=index))\n",
    "\n",
    "    print(\"Optotype = {x}\".format(x=y_train_optotype[index]))\n",
    "    print(\"Angle = {x}\".format(x=y_train_angle[index]))\n",
    "    print(x_test[index])\n",
    "\n",
    "    ax.set_title(\"Train - {x}\".format(x=y_train_optotype[index]))\n",
    "    ax.imshow(x_train[index])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "print_random_test_values(ax[0])\n",
    "print_random_training_values(ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "## RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO : Going to try this experiment with this guy https://github.com/EscVM/Efficient-CapsNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def testicle_test():\n",
    "    y_train_angle = np.zeros(27, dtype=np.int)\n",
    "    for i, sample_name in enumerate(low_distortion_filenames):\n",
    "        thing = extract_angle(sample_name)\n",
    "        y_train_angle[i] = thing\n",
    "    print(np.unique(y_train_angle))\n",
    "\n",
    "testicle_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transfer learning with VGG16\n",
    "\n",
    "### Create the model\n",
    "- Make sure `model.trainable = False `\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "training_set = image_dataset_from_directory(\"/Users/brookeryan/Developer/BaldiLab/Visual-Acuity/images/training/\",\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=32,\n",
    "                                             image_size=(150, 150))\n",
    "val_dataset = image_dataset_from_directory(\"/Users/brookeryan/Developer/BaldiLab/Visual-Acuity/images/testing/\",\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=32,\n",
    "                                                  image_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_base_model():\n",
    "    vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(h, w, 3))\n",
    "    vgg.trainable = False\n",
    "    return vgg\n",
    "\n",
    "base_model = create_base_model()\n",
    "x = base_model(x_train, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "inputs = keras.Input(shape=(h, w, 3))\n",
    "outputs = keras.layers.Dense(1, activation=tf.keras.activations.softmax)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(training_set, epochs=20, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import  models\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# !pip install pydot\n",
    "# !pip install graphviz\n",
    "# !pip install pydotplus\n",
    "\n",
    "model = tf.keras.applications.VGG16()\n",
    "# plot_model(model)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras import  models\n",
    "\n",
    "base_model = VGG16(weights='imagenet')\n",
    "model_VGG16 = models.Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output)\n",
    "\n",
    "model_VGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG16(include_top=False, weights=\"imagenet\", input_shape=(h, w, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:45]:\n",
    "    layer.trainable = False\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "\n",
    "def create_transfer_model_2():\n",
    "    input_t = K.Input(shape=(h, w, 3))\n",
    "    res_model = K.applications.ResNet50(include_top=False,\n",
    "                                        weights=\"imagenet\",\n",
    "                                        input_tensor=input_t)\n",
    "\n",
    "    for layer in res_model.layers[:45]:\n",
    "        layer.trainable = False\n",
    "    #to_res = (224, 224)\n",
    "    model = K.models.Sequential()\n",
    "    #model.add(K.layers.Lambda(lambda image: tf.image.resize(image, to_res)))\n",
    "    model.add(res_model)\n",
    "    model.add(K.layers.Flatten())\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(256, activation='relu'))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(128, activation='relu'))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(64, activation='relu'))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(4, activation='softmax')) #this\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_transfer_model_2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-7f7be0b7",
   "language": "python",
   "display_name": "PyCharm (DeepRecognition)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}